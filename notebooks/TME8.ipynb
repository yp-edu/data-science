{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf788c0-4a87-427a-9e82-33d890c3e90a",
   "metadata": {},
   "source": [
    "# Lab: machine learning protocol, part 2\n",
    "\n",
    "Author: Alasdair Newson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d6677-84ba-4679-bb1e-a270d171e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# data functionalities\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_moons, make_circles,  make_blobs,fetch_covtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# pipeline and metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# pour eviter les warnings embetants\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5df37f-2dc4-449d-933b-8880a5c18447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918dabe-57db-4092-97e0-e8cfb32df398",
   "metadata": {},
   "source": [
    "# 1/ Common machine learning models\n",
    "\n",
    "In this section, we look at some common machine learning algorithms, and compare them. First, we start by creating some toy data with the ```make_classification``` function of scikit-learn (you can use this to test out algorithms for your projects). This creates toy data using Gaussian clusters with a certain covariance (automatically imposed). Among all the features, some are acutally useful ```n_informative```, some are redundant ```n_redundant``` (linear combinations of the useful features), some are repeated ```n_repeated```, and finally the rest are random features. You can specify the number of classes with ```n_classes```.\n",
    "\n",
    "We create the data now (code given):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06479712-8388-4815-9ab5-e0b3b4974b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1. Generate toy tabular data ====\n",
    "# Binary classification with 20 features, some informative, some noisy\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_classes=2,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_repeated=0,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=1.5,\n",
    "    random_state=random_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bfd91-9f0c-4f11-a0a8-ab31fc7b47b8",
   "metadata": {},
   "source": [
    "Now, split the data into train and test. Use $80\\%$/$20\\%$ split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4a551-7d82-4686-96d1-e1fa39f739e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e318990-c172-433d-94c7-b204a0ede836",
   "metadata": {},
   "source": [
    "Now, we will test some common machine learning models and compare their performances.\n",
    "\n",
    "First we will start with the SVM. We will try three versions : \n",
    "- standard soft margin SVM\n",
    "- soft margin SVM with Polynomial Kernel\n",
    "- soft margin SVM with Gaussian Radial Basis Function Kernel\n",
    "\n",
    "Beforet this, we will normalise data. This is important in particular in the case of the Gaussian Radial Basis Function. Indeed, it is based on the distance between two points (see slides), so if the scale of one is much larger, it will dominate the classification. To make the scaling/classification process more automatic, we can use the ```Pipeline``` functionality of scikit-learn. The syntax is, for example:\n",
    "- ```Pipeline([(\"scaler\", ...), (\"svm\", ...)])```\n",
    "The ```...``` parts must be filled in with the scaling and classification functions you wish to use.\n",
    "\n",
    "Do this now for the three cases above. You can use the svm documentation to help you:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709dac4-02eb-4e8f-8a8b-9dec6d32ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "linear_svm = Pipeline([  (\"scaler\", ...),\n",
    "        (\"linear_svm\", ...)])\n",
    "\n",
    "poly_svm = Pipeline([  (\"scaler\", ...),\n",
    "        (\"poly_svm\", ...)])\n",
    "\n",
    "rbf_svm = Pipeline([  (\"scaler\", ...),\n",
    "        (\"rbf_svm\", ...)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1498571-f9cd-4b8e-96c3-814cf8c548f4",
   "metadata": {},
   "source": [
    "Now, train and test these models using the data above. Use the ```accuracy_score``` function to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587fd33-4622-48a9-9244-8b2c506b1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e75fb-4d4a-4b78-95bb-b3581552534e",
   "metadata": {},
   "source": [
    "What are your conclusions ? Let's make the data more complicated now (\"circles\" data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eb13f-bb44-49d2-a1ba-b1dae661fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=2000, factor=0.7, noise=0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f05ee7-6a3c-4697-a777-7a79ba26bab9",
   "metadata": {},
   "source": [
    "Carry out the classification with the new dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01723f-2920-4296-87da-1c6d999cb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201632cd-e877-4d14-a3c9-64e48d53ba74",
   "metadata": {},
   "source": [
    "Now, let's try some other models:\n",
    "- random forests : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- gradient boosting : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "Use the following hyper-parameters:\n",
    "\n",
    "For random forests:\n",
    "- ```n_estimators=300```\n",
    "- ```max_depth=None```\n",
    "For gradient boosting:\n",
    "- ```n_estimators=300```\n",
    "- ```max_depth=3```\n",
    "- ```learning_rate=0.05```\n",
    "\n",
    "Note, you do not need to use the scaling and ```pipeline``` for these models. This is because decision trees make their decisions based on splits (thresholds), which thus only depend on an ordering of a variable, not the scale in itself (although you can try the scaling if you wish to see if it performs better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571a36b-d38f-46f6-91ec-ed98229f9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87373239-f187-4d19-a15c-980ef28cf34d",
   "metadata": {},
   "source": [
    "Conclusions on comparisons of SVM, random forest and gradient boosting ? Why do you think you observe these results ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c5e4d-a172-4ba7-8a18-4d12dc22b831",
   "metadata": {},
   "source": [
    "ANSWER : SVM with Gaussian RBF works better because these are toy data with a strong geometric meaning (circles), so it is specifically designed for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b4b8e-58f6-4215-b2f4-0293cb391468",
   "metadata": {},
   "source": [
    "## 1.1 Real-world datasets\n",
    "\n",
    "The previous datasets were toy datasets, with a strong geometric meaning (concentric circles) , and synthetic (ie, we generated them on the fly). In the real world, datasets are not usually like this: they are fixed, complex and often are not \"geometrically\" meaningful, because each variable has a semantic meaning (unless there is correlation between variables).\n",
    "\n",
    "Let's turn to some more complex data (still classification): the \"cover-type\" data. This is forestry data, with 7 classes and 54 variables/features. The goal is to classify areas of forests into dominant tree species (spruce, aspen etc):\n",
    "- https://archive.ics.uci.edu/dataset/31/covertype\n",
    "The variables are quantities such as elevation etc. First, let's load the data and carry out train/test split:\n",
    "\n",
    "Since the dataset is quite large, we will take a subset of the data for this lab work, to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fa6a9-d89f-44bd-944b-d2cc4285f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = fetch_covtype(return_X_y=True)\n",
    "# reduce dataset size\n",
    "n = 3000\n",
    "X = X[0:n,:]\n",
    "y = y[0:n]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd34fd-39d1-46a4-ac18-1ac09226c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b9916-0ada-407a-ba6a-6d60ca4e2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448b432-70ca-4b88-9472-be73387cc8de",
   "metadata": {},
   "source": [
    "Compare the performances and the execution time of all the models (3 SVMs, random forest, gradient boosting). What are the advantages and disadvantages ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4204e-45c2-41d0-ac4d-00c6178b883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f2bc7-6d47-4908-8262-bb8cfe665d6b",
   "metadata": {},
   "source": [
    "Conclusions on this new dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03215ae-9b2c-4d45-a52f-991441a37685",
   "metadata": {},
   "source": [
    "## 2/ Evaluation metrics: Precision, recall, f1\n",
    "\n",
    "\n",
    "As we saw in class, in binary classification problems we use the notions of accuracy, precision, recall, and the F1 score. Let us recall their definitions:\n",
    "\\begin{equation}\n",
    "    \\text{accuracy} = \\frac{ \\sum_i{ \\textbf{1}_{\\hat{Y}_i=Y_i}}}{n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{precision} = \\frac{ \\sum_i{ \\textbf{1}_{\\hat{Y}_i=1 \\& Y_i=1}}}{\\sum_i{ \\textbf{1}_{\\hat{Y}_i=1}}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{recall} = \\frac{ \\sum_i{ \\textbf{1}_{\\hat{Y}_i=1 \\& Y_i=1}}}{\\sum_i{ \\textbf{1}_{Y_i=1}}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{F1} = 2*\\frac{precision * recall}{precision+recall}\n",
    "\\end{equation}\n",
    "\n",
    "We also define the __confusion matrix__:\n",
    "\n",
    "| **Label / Prediction** | **Negative**        | **Positive**        |\n",
    "| ---------------------- | ------------------- | ------------------- |\n",
    "| **Negative**           | TN   | FP |\n",
    "| **Positive**           | FN | TP |\n",
    "\n",
    "\n",
    "With :\n",
    "\n",
    "- TN: number of true negatives\n",
    "- FN: number of false negatives \n",
    "- FP: number of false positives\n",
    "- TP: number of true positives\n",
    "\n",
    "In this section, we will compute these different elements. To illustrate these concepts, we will use data from the circles dataset:\n",
    "```\n",
    "X, Y = make_circles(n_samples=n, factor=0.7, noise=0.1, random_state=0)\n",
    "```\n",
    "\n",
    "Create the data with $n=300$ and the other arguments as above. Display them using a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e8157-8675-47e5-b280-a3078d374b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179ec43-38cf-418c-a3e6-29f765070a70",
   "metadata": {},
   "source": [
    "Split into train/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4b208-875a-480b-ba9b-55176155f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926dc76-db8f-4558-a108-1ba85d2c841b",
   "metadata": {},
   "source": [
    "Now we choose a classification algorithm. To change up a bit, let's use another algorithm, the k-nearest neighbours. Implement this now with $k = 20$. The syntax is:\n",
    "\n",
    "```\n",
    "neigh = KNeighborsClassifier(n_neighbors=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb475d-0d9c-4d1c-8b0d-72bab5968af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf118b-dc09-48cf-97dc-660827e2dae0",
   "metadata": {},
   "source": [
    "Use the scikit-learn functions (see imports at the beginning) to calculate accuracy, precision, recall and f1 score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb87994-dc5d-43eb-8474-d49408dfe6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38661333-38d1-4084-8721-eb01fcaf4cca",
   "metadata": {},
   "source": [
    "Are these results good ? Let's take a more complicated case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb84e6-edd4-41fb-8ae8-6953a42e01ae",
   "metadata": {},
   "source": [
    "## 2.1 Unbalanced data\n",
    "\n",
    "We will use the \"blobs\" dataset from scikit-learn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1d9f4-a33c-4413-b696-85c9aeb012a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(0, 0), (2, 2)]\n",
    "n = 2000\n",
    "X, y = make_blobs(n_samples=n, centers=centers, shuffle=False, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be326ae-32a9-4a48-b1f1-ad8458c0dac7",
   "metadata": {},
   "source": [
    "Display these data. Do you think the separation will be as easy as before ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46dff5-e4cc-4af0-9ba0-7f3d7745446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e66ac-c79c-484e-945c-dad175316b8f",
   "metadata": {},
   "source": [
    "Now, we will remove a large portion of the data labeled as positive (label = 1). Since the first $n/2$ samples are labeled 0 and the following ones are labeled 1 (because we set shuffle=False), we simply need to remove samples from the end of $X$ and $y$.\n",
    "\n",
    "Remove $\\tau=90\\%$ of the positive samples from the dataset. To do this, you can create new variables called ```X_reduced``` and ```y_reduced```. Then, display the reduced dataset to check that the operation worked correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c12710-5eda-413c-8a1f-0a4f86f18905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188187d1-5c2a-445b-94ae-9e7211d93bbb",
   "metadata": {},
   "source": [
    "Now, carry out the k-nearest-neighbours algorithm again on this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2eb799-ed5d-411a-ab51-9dbe00d26f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe6915-a4b3-4a84-a0b2-836d462c4b3d",
   "metadata": {},
   "source": [
    "How would you interpret these results if you had to explain them to someone ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e435136-8c09-4979-80a8-b21ce7590011",
   "metadata": {},
   "source": [
    "ANSWER : We got a good precision, so when we predict positive (class 1), we are quite sure. However, the recall is bad, so we missed quite a few positive points. This is quite normal: since we removed a lot of positive points, and put the data close together, we can only detect the positive points which are furthest away from the negative points. The ones which are mixed up have no good nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29e63a-6881-4c05-9e5e-a25ae3787106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
